{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 4. 스파크 SQL과 데이터 프레임: 내장 데이터 소스 소개\n",
    "#### 작성자: 이현준(6기 러너)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Session 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession\n",
    "      .builder\n",
    "      .appName(\"Chatper4\")\n",
    "      .getOrCreate())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark.sql 예제 88p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로\n",
    "csv_file = \"PySpark/data/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "\n",
    "# 읽고 임시뷰를 생성\n",
    "# 스키마 추론(더 큰 파일일 경우에는 스키마를 지정해주자)\n",
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .load(csv_file))\n",
    "\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비행거리가 1,000마일 이상인 모든 항공편 조회 90p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT distance, origin, destination\n",
    "    FROM us_delay_flights_tbl WHERE distance > 1000\n",
    "    ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 샌프란시스코(SFO)와 시카고(ORD) 간 2시간 이상 지연이 있었던 모든 항공편 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "    FROM us_delay_flights_tbl\n",
    "    WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "    ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모든 미국 항공편에 대해 지연에 대한 표시를 레이블로 지정 91p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "              CASE\n",
    "                  WHEN delay > 360 THEN 'Very Long Delays'\n",
    "                  WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "                  WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "                  WHEN delay > 0 and delay < 60  THEN  'Tolerable Delays'\n",
    "                  WHEN delay = 0 THEN 'No Delays'\n",
    "                  ELSE 'Early'\n",
    "               END AS Flight_Delays\n",
    "               FROM us_delay_flights_tbl\n",
    "               ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이썬 데이터 프레임 예제 \n",
    "### 비행거리가 1,000마일 이상인 모든 항공편 조회 92p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    " .where(col(\"distance\") > 1000)\n",
    " .orderBy(desc(\"distance\"))).show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 샌프란시스코(SFO)와 시카고(ORD) 간 2시간 이상 지연이 있었던 모든 항공편 des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "(df.select(\"date\", \"delay\", \"origin\", \"destination\")\n",
    " .where((col(\"delay\") > 120) & (col(\"origin\") == 'SFO') & (col(\"destination\") == \"ORD\"))\n",
    " .orderBy(desc(\"delay\"))).show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모든 미국 항공편에 대해 지연에 대한 표시를 레이블로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"date\", \"delay\", \"destination\",\n",
    "          when(df.delay > 360, \"Very Long Delays\")\n",
    "          .when((df.delay >= 120) & (df.delay <= 360), \"Long Delays\")\n",
    "          .when((df.delay >= 60) & (df.delay < 120), \"Short Delays\")\n",
    "          .when((df.delay > 0) & (df.delay < 60), \"Tolerable Delays\")\n",
    "          .when(df.delay == 0, \"No Delays\")\n",
    "          .otherwise(\"Early\").alias(\"Flight_Delays\")\n",
    "          ).orderBy(\"origin\", df.delay.desc()).show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL 데이터베이스와 테이블 생성하기 93p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learn_spark_db라는 데이터베이스 생성 및 사용 선언하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// In Scala/Python\n",
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 관리형 테이블 생성 94p (기존 코드 오류로 인한 아래 코드로 수정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # // In Scala/Python 기존 코드\n",
    "# spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오류로 인한 수정한 코드\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Chatper4\").config(\"spark.sql.legacy.createHiveTableByDefault\", \"false\").getOrCreate()\n",
    "#// In Scala/Python\n",
    "spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT,distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 관리형 테이블 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "# Path to our US flight delays CSV file\n",
    "csv_file = \"PySpark/data/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\" # Schema as defined in the preceding example\n",
    "schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\" \n",
    "flights_df = spark.read.csv(csv_file, schema=schema) \n",
    "\n",
    "# 테이블명이 같은 테이블은 오류를 발생하기 때문에 여기서는 삭제하고 진행\n",
    "# 위에서 만든 테이블 삭제\n",
    "spark.sql(\"DROP TABLE IF EXISTS managed_us_delay_flights_tbl\")\n",
    "flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비관리형 테이블 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 만든 테이블 삭제\n",
    "spark.sql(\"DROP TABLE IF EXISTS us_delay_flights_tbl\")\n",
    "\n",
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT,distance INT, origin STRING, destination STRING)\n",
    "    USING csv OPTIONS (\n",
    "    PATH \"PySpark/data/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\")\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 덮어쓰기\n",
    "spark.conf.set(\"spark.sql.legacy.allowNonEmptyLocationInCTAS\", \"true\")\n",
    "\n",
    "# 위에서 만든 테이블 삭제\n",
    "spark.sql(\"DROP TABLE IF EXISTS us_delay_flights_tbl\")\n",
    "\n",
    "# 데이터 프레임 API에서 사용해야하는 명령어\n",
    "(flights_df\n",
    "      .write\n",
    "      .option(\"path\", \"/tmp/data/us_flights_delay\")\n",
    "      .saveAsTable(\"us_delay_flights_tbl\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임시뷰 예제 95p\n",
    "### 미국 비행 지연 데이터에서 뉴욕이 출발지인 공항이 있는 하위 데이터셋에 대해서만 작업하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- In SQL\n",
    "spark.sql(\"\"\"CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE origin = 'SFO'\"\"\")\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE origin = 'JFK'\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 96p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
    "# Create a 전역 임시 and 임시 뷰\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전역 임시뷰는 접두사를 사용, 임시뷰는 접두사 없이 접근 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- In SQL\n",
    "spark.sql(\"\"\"SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view\"\"\")\n",
    "spark.sql(\"\"\"SELECT * FROM us_origin_airport_JFK_tmp_view\"\"\")\n",
    "# // In Scala/Python\n",
    "spark.read.table(\"us_origin_airport_JFK_tmp_view\")\n",
    "# // Or\n",
    "spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 뷰 드롭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -- In SQL\n",
    "# DROP VIEW IF EXISTS us_origin_airport_SFO_global_tmp_view; \n",
    "# DROP VIEW IF EXISTS us_origin_airport_JFK_tmp_view\n",
    "\n",
    "# // In Scala/Python\n",
    "spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메타테이터 보기 97p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  // In Scala/Python\n",
    "spark.catalog.listDatabases()\n",
    "spark.catalog.listTables()\n",
    "spark.catalog.listColumns(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 캐싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -- In SQL\n",
    "# # 테이블에 캐싱하기\n",
    "# CACHE [LAZY] TABLE <table-name> \n",
    "# UNCACHE TABLE <table-name>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테이블을 데이터 프레임으로 읽기 98p\n",
    "### SQL을 사용하여 테이블을 쿼리하고 반환된 결과를 데이터프레임에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_flights_df = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\n",
    "us_flights_df2 = spark.table(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 프레임 및 SQL 테이블을 위한 데이터 소스\n",
    "### DataFrameReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrameReader.format(args).option(“key”,”value”).schema(args).load()\n",
    "# SparkSession 인스턴스를 통해서만 DataFrameReader에 엑세스 할 수 있다.\n",
    "\n",
    "# SparkSession.read # 정적 데이터 소스\n",
    "# SparkSession.readStream # 스트리밍 소스"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrameWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrameWriter.format(args)\n",
    "# \t.option(args)\n",
    "# \t.bucketBy(args)\n",
    "# \t.partitionBy(args)\n",
    "# \t.save(path)\n",
    "\n",
    "# DataFrameWriter.format(args).option(args).bucketBy(args).partitionBy(args).save(path)\n",
    "\n",
    "# 위에서 설명한 read와 같다.\n",
    "# DataFrame.write \n",
    "# // or \n",
    "# DataFrame.writeStream"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파케이 파일 읽어오기 102p\n",
    "### 데이터프레임으로 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "file = \"\"\"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\"\"\"\n",
    "df = spark.read.format(\"parquet\").load(file)\n",
    "df.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL 테이블로 읽기 103p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- In SQL\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl \n",
    "    USING parquet \n",
    "    OPTIONS (\n",
    "        path \"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\" )\n",
    "\"\"\")\n",
    "\n",
    "# In Python\n",
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)\n",
    "\n",
    "# # DataFrame으로 테이블을 읽어옵니다.\n",
    "# us_delay_flights_df = spark.read.format(\"parquet\").option(\"path\", \"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\").load()\n",
    "\n",
    "# # 임시 뷰를 생성합니다.\n",
    "# us_delay_flights_df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 프레임을 파케이 파일로 쓰기 104p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "(df.write.format(\"parquet\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"compression\", \"snappyb\")\n",
    "    .save(\"/tmp/data/parquet/df_parquet\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스파크 SQL 테이블에 데이터 프레임 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "(df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"us_delay_flights_tbl\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON 파일을 데이터 프레임으로 읽기 105p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "file = \"/Users/hyunjun/vscode/Spark//databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\n",
    "df = spark.read.format(\"json\").load(file)\n",
    "df.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스파크 SQL 테이블로 JSON 파일 읽기 106p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- In SQL\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl USING json\n",
    "    OPTIONS (\n",
    "    path \"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "#// In Scala/Python\n",
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 데이터 프레임을 JSON 파일로 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "(df.write.format(\"json\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .save(\"/tmp/data/json/df_json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV 파일을 데이터 프레임으로 읽기 108p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "file = \"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\"\n",
    "schema = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\"\n",
    "df = (spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")# Exit if any errors .option(\"nullValue\", \"\")\n",
    "    .schema(schema)# Replace any null data field with quotes \n",
    "    .option(\"mode\", \"FAILFAST\")\n",
    "    .load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- In SQL\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl USING csv\n",
    "OPTIONS (\n",
    "path \"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\", header \"true\",\n",
    "inferSchema \"true\",\n",
    "mode \"FAILFAST\"\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# // In Scala/Python\n",
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 프레임을 CSV 파일로 쓰기 109p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/data/csv/df_csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에이브로 110p\n",
    "### 에이브로 파일을 데이터 프레임으로 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"avro\").load(\"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에이브로 파일을 스파크 SQL 테이블로 읽기 111p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- In SQL\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW episode_tbl USING avro\n",
    "OPTIONS (\n",
    "path \"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\"\n",
    ")\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테이블 생성 후 SQL로 데이터 프레임 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "spark.sql(\"SELECT * FROM episode_tbl\").show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 프레임을 에이브로 파일로 쓰기 112p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "(df.write\n",
    "  .format(\"avro\")\n",
    "  .mode(\"overwrite\")\n",
    "  .save(\"/tmp/data/avro/df_avro\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORC 파일을 데이터 프레임으로 읽기 113p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "file = \"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\n",
    "df = spark.read.format(\"orc\").option(\"path\", file).load()\n",
    "df.show(10, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스파크 SQL 테이블로 ORC 파일 읽기 114p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- In SQL\n",
    "spark.sql(\n",
    "    \"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl \n",
    "    USING orc\n",
    "    OPTIONS (\n",
    "    path \"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\n",
    "    )\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# // In Scala/Python\n",
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 프레임을 ORC 파일로 쓰기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "(df.write.format(\"orc\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .save(\"/tmp/data/orc/flights_orc\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 파일을 데이터 프레임으로 읽기 116p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "from pyspark.ml import image\n",
    "image_dir = \"PySpark/data/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\n",
    "images_df = spark.read.format(\"image\").load(image_dir)\n",
    "images_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df.select(\"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\",\n",
    "      \"label\").show(5, truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 프레임으로 이진 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "path = \"PySpark/data/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\n",
    "binary_files_df = (spark.read.format(\"binaryFile\")\n",
    "    .option(\"pathGlobFilter\", \"*.jpg\")\n",
    "    .load(path))\n",
    "binary_files_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파티션 데이터 검색 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "binary_files_df = (spark.read.format(\"binaryFile\")\n",
    "    .option(\"pathGlobFilter\", \"*.jpg\")\n",
    "    .option(\"recursiveFileLookup\", \"true\")\n",
    "    .load(path))\n",
    "binary_files_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15829a5d5b0063a6cb8d1011ad4433a337dc97bb73b11e2356c031cceb4e0399"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
