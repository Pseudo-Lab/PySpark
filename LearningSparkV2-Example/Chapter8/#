#chap 8

### 아파치 스파크의 스트림 처리 엔진의 진화

- 스트림 처리: 끝없이 들어오는 데이터 흐름을 연속적으로 처리하는 것
    - 빅데이터의 등장: 단일 노드 처리 엔진 → 멀티 노드 분산 처리 엔진
- 레코드 단위 처리 모델: 전통적인 분산 스트림 처리
    
    - 처리 파이프라인은 각 노드들의 지향성 그래프로 구성
    - 각 노드는 지속적으로 한번에 하나씩 레코드를 받고 처리하여 생성된 레코드를 다음 노드로 전송 → 매우 짧은 응답시간
    - But, 특정 노드가 장애를 겪거나 다른 노드보다 느린 상황에서 회복하는 것에 효과x
        - 많은 복구 자원을 써서 빨리 복구하는 것이 아니라면, 최소한의 복구 자원으로 느리게 복구됨

### 마이크로 배치 스트림 처리의 출현

- 스파크 스트리밍(or DStream) == 마이크로 배치 스트림 처리
- 스트리밍 처리를 아주 작은 맵리듀스 스타일 배치 처리 잡 형태로써 작은 스트리밍 데이터 조각들에 대해 연속적으로 수행하는 모델


- 입력 스트림에서 데이터를 잘게 쪼개어, 각 배치는 분산 처리 방식으로 스파크 클러스터에서 각 마이크로 배치의 결과를 생성하는 작은 태스크들을 실행
- 마이크로 배치 스트림의 장점 (전통적인 분산 스트림 대비)
    - 스파크의 기민한 태스크 스케줄링은 매우 빠르고 효과적으로 태스크를 다른 이그제큐터들로 복제해 실행하게 함으로써 이그제큐터 장애나 속도 저하에 대응 가능
    - 통제된 태스크의 할당은 태스크가 여러 번 실행되더라도 동일한 결과 → 중복없는 일회 처리를 보장 (모든 입력 레코드에 대해 정확히 한 번만 처리된 결과)
- 효과적인 장애 대응 특징은 반응 속도에 대한 비용이 필요하지만, 마이크로 배치 모델은 밀리초수준의 반응 속도를 보여줄 수는 없음 → But, 초단위 반응 속도의 단점을 상쇄하는 배치 처리 장점을 가짐
    - 웬만한 파이프라인은 굳이 초 단위 이하의 반응 속도를 필요로 하지 않음
    ex) 스트리밍 결과가 시간 단위로 실행되는 배치 잡에 의해 실행된다면, 초 단위 이하의 반응 속도가 결과 생성에 도움되지 않음
    - 파이프 라인의 다른 부분에서 더 큰 지연이 있는 경우가 많음
- DStream API는 스파크 배치 RDD API 기반으로 작성됨
    - 따라서, D스트림은 RDD와 동일한 기능적 의미, 동일한 장애 복구 모델로서 동작함
    - 즉, 스파크 스트리밍은 배치와 스트리밍, 인터액티브한 워크로드들을 하나의 일체화된 처리 엔진을 통해 안정적인 API와 개념을 바탕으로 제공할 수 있음

스파크 스트리밍(D스트림)에서 얻은 교훈

1. 배치나 스트림 처리를 위한 단일 API 부재
2. 논리/물리 계획 간에 구분 부족
3. 이벤트 타임 윈도우를 위한 기본 지원 부족

정형화 스트리밍의 철학

1. 배치 및 스트림 처리를 위한 단일화된 프로그래밍 모델과 인터페이스
2. 더 넓은 범위의 스트리밍 처리 지원

정형화 스트리밍은 자동적으로 배치 스타일을 쿼리를 스트리밍 실행 계획으로 변환 → 증식화

매번 레코드가 도착할 때 마다 결과를 업데이트 해주기 위해 어떤 것이 필요한지 파악하고 있다.

정형화 스트리밍의 3가지 결과 모드

1. 추가모드 - 지난 트리거 이후로 결과 테이블에 새로 추가된 행들만 외부 저장소에 쓰임
2. 갱신모드 - 지난 트리거 이후에 결과 테이블에서 갱신된 행들만 외부 저장소에서 변경
3. 전체모드 - 갱신된 전체 테이블이 외부 저장소에 쓰임


- 데이터 스트림을 테이블로 생각하면, 데이터에 대한 논리적 연산을 개념화하기 더 쉽게 만들어 주고, 코드 표현도 더 쉬움
- 스파크의 데이터 프레임은 프로그래밍 방식의 테이블 표현이기 때문에 데이터 프레임 API를 스트리밍 데이터의 연산을 표현하는 데에 쓸 수 있음.
- 스트리밍 데이터 소스로부터 입력 데이터 프레임을 정의하고, 배치 소스에서 정의된 데이터 프레임에 하는 것과 같은 방식으로 데이터 프레임 연산을 정의해주면 됨.

### 정형화 스트리밍 쿼리의 기초

정형화 스트리밍과 배치 쿼리에서 데이터 처리 로직 표현에 쓰는 데이터 프레임 API에는 핵심적인 차이가 있음

### 스트리밍 쿼리를 정의하는 데 필요한 다섯 단계

### 1단계: 입력 소스 지정

배치 데이터 소스에서 읽어 들일 때는 DataFrameReader 객체를 만들어주는 spark.read를 썼지만, 스트리밍 소스에 대해서는 DataStreamReader를 만들어주는 spark.readStream을 사용해야 함

- DataStreamReader는 DataFrameReader와 동일한 함수들을 대부분 갖고 있음

```python
# 파이썬 예제
spark = SparkSession…

lines = (spark.readStream.format(“socket”)

.option(“host”,”localhost”)

.option(“port”,9999)

.load())
```

localhost:9999에서 개행 문자로 구분되는 텍스트 데이터를 읽어 무한 테이블 형태의 lines 데이터 프레임을 생성

- spark.read를 쓸 때처럼 스트리밍 데이터를 즉시 읽어 들이는 것은 아님
- 위 코드는 실제로 스트리밍 쿼리가 동작할 때 데이터를 읽어들이기 위해 필요한 설정들을 정의하는 것일 뿐
- 소켓 외에도 아파치 스파크는 그 외 DataFrameReader가 지원하는 다양한 파일 기반 포맷(파케이, ORC, JSON 등)에서 데이터 스트림을 읽어들일 수 있게 지원
- 스트리밍 쿼리는 유니언이나 조인같은 데이터 프레임 연산을 써서 조합하는 식으로 다중 입력 소스를 지정할 수도 있음

### 2단계: 데이터 변형

```python
# 파이썬 예제
from pyspark.sql.functions import *

words = lines.select(split(col(“value”), “\\s”).alias(“word”))

counts = words.groupBy(“word”).count()
```

- counts 변수는 실행 중인 단어 세기 프로그램을 나타내는 ‘스트리밍 데이터 프레임’이며 한번 스트리밍 쿼리가 시작되고 스트리밍 입력이 지속적으로 처리되면서 계산을 수행하게 됨
- lines  스트리밍 데이터 프레임에 변형을 수행하는 이 연산들은 lines가 일반적인 배치 데이터 프레임이었다 하더라도 정확히 동일한 방식으로 동작함
- 일반적으로 배치 데이터 프레임에 쓰이는 대부분의 데이터 프레임은 스트리밍 데이터 프레임에도 적용될 수 있음

### <두가지 넓은 종류의 데이터 트랜스포메이션>

**상태가 없는 트랜스포메이션**

select(), filter(), map() 같은 연산들은 다음 행 처리를 위해 이전 행 정보를 필요로 하지 않음

각각의 행은 그 자체만으로 처리가 가능

이전 ‘상태’ 정보에 대한 요구가 없기 때문에 무상태 처리가 가능하며, 이런 무상태 연산들은 배치와 스트리밍 데이터 프레임 양쪽에서 사용할 수 있음

**상태정보 유지 트랜스포메이션**

count() 같은 집계 연산은 여러 행에 걸쳐서 데이터가 정보를 유지하고 있기를 요구

그룹화, 조인, 집계 연산과 관계되는 모든 연산들

대부분 정형화 스트리밍에서 지원되지만, 이 중 일부의 조합은 연산이 어렵다거나 증분 방식으로 계산하는 것이 실행 불가능하다는 이유로 지원되지 않음

### 3단계: 출력 싱크와 모드 결정

데이터 변형 후 DataFrame.writeStream을 써서 처리된 출력 데이터가 어떻게 쓰일지 정할 수 있다.

- 자세한 출력 방식(어디에 어떻게 출력 결과가 쓰일지)
- 자세한 처리 방식(어떻게 데이터가 처리되고, 장애 시 어떻게 복구되는지)

출력 방식에 대해 먼저 살펴보면 아래 코드는 최종 집계를 콘솔 화면에 어떻게 출력할지 보여준다.

```python
# In Python
writer = counts.writeStream.format("console").outputMode("complete")
```

위 코드는 출력 스트리밍 싱크에 console, 출력 모드로는 complete를 지정

새로운 입력 데이터의 일부가 들어오고 단어 개수가 갱신되면 지금까지 나온 모든 단어의 개수를 콘솔에 출력할지 아니면 지난 입력 데이터 이후로 갱신된 단어만 대상으로 할지 선택할 수 있다.

모드는 아래 세가지 중에 하나이다.

- 추가 모드(기본 모드)
싱크에 쓰인 마지막 트리거 이후 새로 추가된 행들이 결과 테이블 혹은 데이터 프레임에 쓰인다.
출력된 모든 행이 이후의 쿼리에 의해 변경되지 않는다는 것을 보장한다.
- 전체 모드
결과 테이블 혹은 데이터 프레임의 모든 행이 매번 트리거될 때 마지막에 출력 대상이 된다.
결과 테이블이 입력 테이블에 비해 현저히 작을 때만 지원된다.
- 업데이트 모드
지난 트리거 이후 결과 테이블이나 데이터 프레임에서 변경된 행들만이 매 트리거의 마지막에 출력 대상이 된다.
추가 모드의 반대 동작이라고 할 수 있다.

콘솔에 출력하는 것 말고도 정형화 스트리밍은 기본적으로 스트리밍 파일과 아파치 카프카에 쓰는것을 지원하며 foreachBatch(), foreach() API룰 서용하여 임의의 위치에 쓰는것도 가능하다.

이 싱크들과 지원되는 옵션에 대한 자세한 것은 뒤에서 다룬다.

### 4단계: 처리 세부사항 지정

쿼리 시작 전의 마지막 단계는 데이터를 어떻게 처리할지 세부사항을 지정하는 것이다.

```python
# In Python
checkpointDir = '/Users/hyunjun/vscode/Spark/spark-3.3.2-bin-hadoop3/Chapter8'
writer2 = (writer
    .trigger(processingTime="1 second")
    .option("checkpointLocation", checkpointDir))
```

DataFrame.writeStream으로 생성한 DataStreamWriter를 써서 지정하는 두 종류의 세부 사항이 있다.

- 트리거링 상세 내용
새롭게 추가된 스트리밍 데이터를 발견하고 처리하는 동작이 언제 발동되는지에 대한 네 가지 옵션 존재
    - 기본(default)
    명시적으로 지정되지 않았으면 앞선 마이크로 배치가  완료되자마자 다음 마이크로 배치가 실행되는 곳부터 데이터를 실행
    - 트리거 간격에 따른 처리 시간
    명시적으로 ProcessingTime 간격 지정 가능하며 쿼리는 이 간격에 따라 마이크로 배치 실행
    - 일회 실행(once)
    하나의 마이크로 배치 실행, 처리 후에 멈춤
    - 연속(continuous)
    마이크로 배치 단위 대신 연속적으로 데이터를 처리(실험적인 모드)
    마이크로 배치 트리거에 비해 매우 빠른 응답성 제공
- 체크포인트 위치(checkpoint location)
진행 중인 데이터 상황을 저장할 수 있는 디렉터리
실패 시 실패한 지점에서 재시작할 수 있게 하는 용도로 사용

### 5단계: 쿼리 시작

모든 것이 결정되면 최종 단계에서 쿼리를 아래와 같이 시작한다.

```python
# In Python
streamingQuery = writer2.start()
```

streamingQuery 변수에 반환된 객체 타입은 활성화된 쿼리이며 쿼리 관리에 사용된다.

start()는 논블로킹 함수이며 백그라운에서 쿼리를 실행하고 호출한 즉시 객체를 리턴한다.

streamingQuery.awaitTermination()를 사용해 스트리밍 쿼리가 끝날 때까지 메인 스레드를 블로킹 할 수 있다.

awaitTermination(timeoutMillis)의 형태로 특정 몇 초간 간격만 대기하도록 할 수 있다.

streamingQuery.stop() 호출을 통해 중지 할 수도 있다.

*Linux명령어를 사용해 스트리밍 서버로 데이터를 전송

- 종합 예제
    - 소켓에서 텍스트 데이터 스트림을 읽어들이고, 단어 갯수를 세어서 콘솔에 출력하는 코드
    
    ```python
    # 파이썬 예제
    from pyspark.sql.functions import *
    
    # 1단계: 입력 소스 지정
    spark = SparkSession...
    lines = (spark
             .readStream.format("socket")
             .option("host", "localhost")
             .option("port", 9999)
             .load())
    
    # 2단계: 데이터 변형
    words = lines.select(split(col("value"), "\\s").alias("word"))
    counts = words.groupBy("word").count()
    checkpointDir = "..."
    
    # 3단계: 출력 싱크와 모드 결정 (전체 모드)
    # 4단계: 처리 세부사항 지정
    streamingQuery = (counts
                      .writeStream
                      .format("console")
                      .outputMode("complete")
                      .trigger(processingTime="1 second")
                      .option("checkpointLocation", checkpointDir)
                      .start())
    
    # 5단계: 쿼리 시작
    streamingQuery.awaitTermination()
    ```
    

### 실행 중인 스트리밍 쿼리의  내부

쿼리가 시작되면 엔진에서는 다음 단계들을 순서대로 실행

- 데이터프레임 연산들은 논리 계획으로 변환됨 → 스파크 SQL이 쿼리 계획을 위한 연산을 추상적으로 표현할 수 있게 함
1. 논리 계획을 분석, 최적화
    - 스파크 SQL이 스트리밍 데이터에 대해 스트리밍 특성에 따라 연속적이고 효과적으로 실행할 수 있는지 확인
2. 스파크 SQL은 아래의 루프를 백그라운드 스레드를 통해 반복 실행
    1. 설정된 트리거 간격마다 스레드는 새 데이터가 있는지 스트리밍되는 인풋 경로(streaming source)를 확인
    2. 새 데이터가 있으면 마이크로 배치로 실행
        - 최적화된 논리 계획으로부터 최적화된 스파크 실행 계획(소스로부터 새로운 데이터를 읽고, 계산을 통해 점진적으로 결과를 업데이트시키고, output에 결과를 작성하는 것까지 포함)이 생성됨
    3. 모든 마이크로 배치마다 특정 분량의 데이터가 처리되고, 관련된 모든 상태가 체크포인트에 저장됨
        - 체크포인트 위치에서 쿼리가 필요할 때마다 정확한 범위를 새로 처리할 수 있게 함
3. 위의 루프는 다음 이유들 중 하나로 질의가 종료되기 전까지 계속
    1. 질의에서 오류 발생 (처리에서 오류가 발생하거나 클러스터 장애 발생)
    2. `streamingQuery.stop()` 호출에 의해 명시적으로 질의 중단
    3. 트리거가 일회 실행(`once`)으로 설정되어 있는 경우
        - 질의는 모든 가능한 데이터를 처리하는 한 번의 마이크로 배치 실행 후 정지

- 정형화 스트리밍의 핵심: 내부적으로 데이터 실행에 스파크 SQL을 사용
    - 스트리밍 처리량을 극대화하기 위해 스파크 SQL의 최적화 실행 엔진의 모든 성능을 끌어다 쓰고, 이것이 성능 효율을 높임

## 정확한 일회 실행을 위한 장애 복구

앞서 살펴본 스파크 정형화 스트리밍(마이크로 배치 처리)의 장점은 중복 없는 일회 처리로 신뢰도 있는 결과를 받을 수 있다는 것이었다.

<aside>
💡 **스트림 처리의 신뢰도(reliability)에 따른 세 가지 보장 방식**
At-most-once(최대 한 번): 데이터 유실이 있을 수 있다. 추천하지 않는 방식
At-least-once(적어도 한 번): 데이터 유실은 없으나 재전송으로 인해 중복이 생길 수 있다. 대부분의 경우 충분한 방식
Exactly-once(정확히 딱 한 번): 데이터가 오직 한 번만 처리되어 유실도 중복도 없다. 모든 상황에 대해 완벽히 보장하기 어렵지만 가장 바라는 방식 보았듯이 스파크 정형화 스트리밍은 Exactly-once(딱 한 번)의 방식을 갖는다.

</aside>

**Exactly-once(정확한 일회 실행)** 보장을 위한 조건

- 재실행 가능한 스트리밍 소스: 지난 마이크로 배치에서 미완료된 데이터 범위를 소스에서 다시 읽을 수 있어야 한다.
- 결정론적 연산: 모든 데이터 변형 결과는 동일한 입력 데이터에 대해 동일해야 한다.
- 멱등성 스트리밍 싱크: 싱크는 재실행된 마이크로 배치를 구분할 수 있어야하고, 재실행으로 중복 쓰기가 발생하면 무시할 수 있어야 한다.

### **배치 재실행 및 관련 장애 복구 방안**

**종료한 스트리밍 쿼리를 재시작하는 방안** 

1. 먼저 새 프로세스 생성을 위해 SparkSession을 생성하고 데이터 프레임을 재정의한다.
2. 처음 쿼리가 시작할 때 사용된 것과 같은 체크포인트 위치로 스트리밍 쿼리를 시작한다.
- 체크포인트란?
    
    스트리밍 애플리케이션의 상태를 주기적으로 저장하여 고장 복구를 도울 수 있는 메커니즘으로 아래 두가지 정보를 저장한다.
    (1) 메타데이터: 스트리밍 애플리케이션의 설정, DStream 연산, 그리고 작업 진행 상황과 같은 메타데이터가 포함된다. 메타데이터는 애플리케이션 복구를 위해 필요한 정보를 제공한다.
    (2) 데이터: 스트리밍 애플리케이션에서 사용되는 RDD의 데이터를 저장한다. 이 데이터는 스트리밍 애플리케이션 복구에 필요한 중간 연산 결과를 포함하며, 스트리밍 애플리케이션 복구 시 이전 상태를 복원하는 데 사용된다.
    

**퀴리 재시작 사이 쿼리를 수정하는 법**

- 데이터 프레임 트랜스포메이션
ex) 입력 중 잘못된 바이트 배열은 무시하도록 추가

```python
from pyspark.sql import *
from pyspark.sql.types import BooleanType
from pyspark.sql.functions import *

### 잘못된 바이트 배열을 무시하기 위한 udf 함수
def isCorruptedUdf(value):
    return False

spark = (SparkSession
         .builder
         .appName("CountWord")
         .getOrCreate())

spark.udf.register("isCorruptedUdf", isCorruptedUdf, BooleanType())

lines = (spark
         .readStream.format("socket")
         .option("host", "localhost")
         .option("port", 9999)
         .load())

# add transformation
filteredLines = lines.filter("isCorruptedUdf(value) = False")

# words = lines.select(split(col("value"), "\\s").alias("word"))
words = filteredLines.select(split(col("value"), "\\s").alias("word"))
counts = words.groupBy("word").count()
checkpointDir = "..."

streamingQuery = (counts
                  .writeStream
                  .format("console")
                  .outputMode("complete")
                  .trigger(processingTime="1 second")
                  .option("checkpointLocation", checkpointDir)
                  .start())

streamingQuery.awaitTermination()
```

- 소스와 싱크 옵션
읽기 스트림과 쓰기 스트림이 변경될 수 있는지는 싱크와 소스의 종류에 따라 다르다.
    
    데이터를 다른 호스트나 포트로 전송하는 스트림은 중간에 바꾸면 안되고, 반대로 콘솔 수준 싱크의 스트림은 수정 가능하다. 
    
    예를 들어 매 트리거마다 100줄씩 출력하는 옵션을 추가하고 싶다면 아래와 같은 구성을 추가할 수 있다.
    
    ```python
    writeStream.format("console").option("numRows", "100"),,,
    ```
    
- 상세 부분 처리
체크포인트 위치는 재시작 사이에 바뀌면 안된다. 
하지만 트리거의 간격 등의 상세 내용들은 장애 내구성을 보장하는 선에서 변경 가능하다.

위의 예시 외에도 쿼리 재시작 사이 변경이 허용되는 것들에 대한 자세한 정보들은 이 [가이드](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing)를 참고하면 된다.

### 동작 중인 쿼리 모니터링하기

서비스 환경에서(스트리밍 파이프라인 실행할 때) 중요한 것 → 상태 체크하는 것 

정형화 스트리밍 = 상태 추적할 수 있는 여러 방법 & 실행 중인 쿼리 통계 수치들 제공

### StreamingQuery

1. 현재 상태 가져오기
2. 현재 수치들 가져오기 : 하나의 쿼리 (처리되었을 때) → 무언가 진행 되어 있길 기대
lastProgress() : 가장 마지막 완료된 마이크로 배치에 대한 정보 되돌려 줌
3. StreamingQuery.status() : 현재 상태 가져오기 (백그라운드 쿼리 스레드가 하고 있는 정보 제공)

### Dropwizard Metrics (통계 정보 발행하기)

1. 라이브러리
2. 유명한 모니터링 프레임워크들에게 (Ganglia, Graphite 등) 수치들 제공 
3. 기본적으로 사용하지 않도록 되어있음 (데이터 양이 거대해 질 수 있어서)
4. 사용 가능하게 할려면 SparkSession 설정에 spark.sql.streaming.metricsEable 을 True로 설정

StreamingQuery.lastProgress()로 볼 수 있는 정보들 중 일부만 Dropwizard Metrics를 통해 제공
⇒ 연속적인 진행 정보 발생하고 싶을 시 자체 제작 리스너 만들어야 함

### StreamingQueryListener

1. 임의 로직을 발행되는 수치에 연속적으로 적용하도록 주입 가능 (이벤트 리스터 인터페이스)
2. 스칼라나 자바에서만 가능

## 스트리밍 데이터 소스와 싱크

정형화 스트리밍 쿼리 기본적인 단계 ⇒ 내장된 데이터 소스와 싱크 알아보기

- SparkSession.readStream() ⇒ 스트리밍 소스에서 데이터 프레임 만듦
- DataFrame.writeStream() ⇒ 결과 데이터 프레임 출력 위치 넣을 수 있음
- format() ⇒ 각각의 경우 소스 타입 결정 할 수 있음

## 파일

- 정형화 스트리밍은 배치 처리에서 지원하는 것과 동일한 포맷의 파일들로부터 데이터 스트림을 읽거나 쓰는 것을 지원(ex. 일반 텍스트 파일, CSV, JSON 등)

### 파일 기반의 정형화 스트리밍 처리 방법

1. **파일에서 읽기**
정형화 스트리밍은 디렉터리에 쓰여진 파일들을 하나의 데이터 스트림으로 간주 할 수 있음

- **반환된 스트리밍 데이터 프레임은 특정 스키마를 갖게 된다.**

**[파일을 이용할 때 기억할 몇가지 키포인트]**

- 모든 파일들은 동일 포맷이어야 하며, 동일 포맷을 가질 것이라 가정
=> 위 가정이 어긋나면 잘못된 파싱 결과가 나오거나(ex. 모든 값이 null이 되는 등) 쿼리가 실패할 것이다.
- 각각의 파일은 디렉터리에서 완전한 하나의 파일로 존재해야한다.
= 읽는 시점에 전체 파일이 읽기 가능해야하며, 수정되거나 업데이트 되면 안된다.
- 처리해야 할 새로운 파일이 여러 개가 있을 때 처리량 제한 등으로 인해 그중 일부만 처리될 수 있으며 가장 빠른 타임스탬프를 갖고 있는 파일들이 먼저 선택된다.

1. **파일에 쓰기**
- 정형화 스트리밍은 읽기에 쓰이는 동일 포맷 파일에 스트리밍 쿼리 결과를 쓸 수 있다.
- 단, 기존 데이터 파일 수정이 쉽지 않음(업데이트나 전체 모드에서 필요)
- 파일을 새로 추가는 것은 쉬우므로 추가 모드만 지원(데이터를 디렉토리에 추가하는 셈)
- 파티셔닝 지원

- 정형화 스트리밍은 디렉터리에 쓰이는 데이터 파일들의 로그를 유지하며 파일 쓰기 할 때 전체적으로 정확한 일회 처리를 보장. 다른 처리 엔진들은 이런 로그의 존재를 알 수 없으므로 동일하게 보장을 제공하지 못할 수 있다.
- 재시작 사이에 결과 데이터 프레임의 스키마를 변경한다면, 결과 디렉터리의 파일들이 서로 다른 스키마를 갖고 섞여 있을 수 있다.

- 아파치 카프카는 발행/구독(Publish/Subscribe) 시스템이며 데이터 스트림의 저장 시스템으로 널리 쓰인다.

### 카프카에서 읽기

- 카프카에서 분산 읽기를 수행하려면 어떻게 소스에 접속하는지 옵션들을 사용해 지정해야 한다.

- 반환된 dataframe은 다음 표 8.1에 있는 스키마를 갖는다.
    - 여러 개의 토픽에서 읽어 오거나 토픽의 패턴을 지정해 읽어 오는 것도 가능하다.
    - 또는 카프카 토픽을 테이블처럼 취급하는 것도 가능하다.

카프카에 결과를 쓰기 위해서는 정형 스트리밍은 결과 dataframe이 몇 가지 지정된 컬럼 이름과 타입을 갖고 있기를 요구한다. (표 8.2 참조)

```python
counts = ...
streamingQuery = (counts.
                  selectExpr("cast(word as string) as key",
                             "cast(count as string) as value")
                  .writeStream
                  .format("kafka")
                  .option("kafka.bootstrap.servers", "host1:port1")
                  .option("topic", "wordCounts")
                  .outputMode("update")
                  .option("checkpointLocation", checkpointDir)
                  .start())
```

### 자체 제작 스트리밍 소스와 싱크

- 이번에는 정형 스트리밍에서 기본적으로 지원하지 않는 저장 스토리지에서 어떻게 읽고 쓰는지에 대해 이야기한다.

### 임의의 저장 시스템에 쓰기

- 스트리밍 쿼리의 결과를 임의의 저장 시스템에 쓰도록 해주는 두 가지 실행 함수인 `foreachBatch()`와 `foreach()`가 있다.
    - `foreach()` : 레코드마다 자체 쓰기 로직을 적용할 수 있다.
    - `foreachBatch()` : 매번 마이크로 배치마다 결과 쓰기에 자신의 로직과 임의의 연산을 적용할 수 있다.

```python
hostAddr = "<ip address>"
keyspaceName = "<keyspace>"
tableName = "<tableName>"

spark.conf.set("spark.cassandra.connection.host", hostAddr)

def writeCountsToCassandra(updatedCountsDF, batchId):
    (updatedCountsDF
     .write
     .format("org.apache.spark.sql.cassandra")
     .mode("append")
     .options(table=tableName, keyspace=keyspaceName)
     .save())

streamingQuery = (counts
                  .writeStream
                  .foreachBatch(writeCountsToCassandra)
                  .outputMode("update")
                  .option("checkpointLocation", checkpointDir)
                  .start())
```

- `foreachBatch()`를 쓰면 다음과 같은 것들이 가능하다.
    - 기존 배치 데이터 소스 재활용
    - 여러 곳에 쓰기

### foreach() 사용

- 만약 적절한 배치 데이터 Writer 객체가 존재하지 않아`foreachBatch()`를 쓸수 없다면, `foreach()` 사용.
- 정형화 스트리밍은 open() , process(), close() 이 세개의 함수로 나눠서 출력 레코드들의 각 파티션에 쓴다.
- 예시코드
    - [공식문서](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreach)

## 1. 함수사용
def process_row(row):
	# 이제 저장장치에 쓴다.
	print("Processing row:", row)

query = streamingDF.writeStream.foreach(process_row).start()


## 2. ForeachWriter 클래스 사용
class ForeachWriter:
	def open(self, partitionId, epochId):
	# 데이터 저장소에 대한 접속을 열어놓는다.
	# 쓰기가 계속되어야 하면 true를 리턴한다.
	# 파이썬에서는 이 함수는 선택 사항이다.
	# 지정되어 있지 않다면 자동적으로 쓰기는 계속될 것이다.
	return true

def process(self,row):
	#열린 접속을 사용해서 저장소에 문자열을 쓴다.
	# 이 함수는 필수
	print("Processing row:", row)

def close(self, error):
	# 접속을 닫는다. 이 함수는 선택
	pass

resultDF.writeStream.foreach(ForeachWriter()).start()

### 데이터 트랜스포메이션

정형화 스트리밍에서 지원하는 데이터 트랜스포메이션을 자세히 알아보자.

앞서 스트리밍 쿼리의 내부를 살펴본 내용은 다음과 같다.

- 스파크 SQL의 카탈리스트 옵티마이저는 모든 데이터 프레임 연산을 최적화하여 논리 계획으로 변환하는 작업을 한다.
- 스파크 SQL 플래너는 연속적인 데이터 스트림 위에서 논리 계획을 어떻게 실행할지 결정한다.
여기서 상황에 따라 논리 계획을 일회성 실행 계획으로 바꾸기도 하고 연속적 실행 계획 묶음을 생성하기도 한다.
- 실행 계획이 실행되어 결과 데이터 프레임을 계속 업데이트 한다.

여기서 각 실행 계획은 하나의 마이크로 배치가 되고, 중간 결과들은 스트리밍 ‘상태’가 된다.

누적 연산을 실행하기 위해 상태 정보가 필요한 경우가 있는데, 
상태 정보가 필요한지 여부에 따라 데이터 프레임 연산을 크게 ‘무상태’와 ‘상태정보유지’ 로 구분한다.

### 무상태 트랜스포메이션

모든 프로젝션 연산(select(), map() 등)과 선택적 연산(filter(), where() 등)은 이전 행에 대한 정보가 없어도 입력 레코드를 개별적으로 처리할 수 있다.

이러한 연산들로 이루어진 스트리밍 쿼리는 무상태 트랜스포메이션에 해당하며 앞서 이미 처리된 결과 레코드는 수정할 수 없기 때문에 추가 및 업데이트 출력 모드는 지원하지만 전체 모드는 지원하지 않는다. 

### 상태 정보 유지 트랜스포메이션

groupby(), count()과 같이 이전 행 정보가 필요한 연산의 스트리밍 쿼리가 여기에 해당된다. 
모든 마이크로 배치에서 누적 계획에 새로운 값과 이전 배치의 값을 더하여 기록한다. 
각 배치의 상태 정보는 스파크 이그제큐터의 메모리에 보관되어 장애복구를 위한 체크포인트 기록을 한다.

이그제큐터는 실행 계획의 실행 결과를 싱크에 기록하는 작업 뿐만 아니라 마이크로 배치 간 상태를 유지하기 위해 중간 데이터를 생성한다. 이 상태 데이터는 파티션되어 분산 처리 되고, 이그제큐터 메모리에 캐싱된다. 

예시로 단어 세기 스트리밍 쿼리에서 상태가 어떻게 관리되는지 살펴보겠다.

1) 각 마이크로 배치는 새로운 단어를 읽어 들인다. 이그제큐터는 각 배치마다 새로 들어온 단어들을 포함하여 셔플 및 그룹화하고, 개수를 세어서 새로운 개수를 출력한다. 또한 다음 마이크로배치를 위해 상태 정보를 이그제큐터의 메모리에 캐싱한다.

2) 다음 마이크로 배치 데이터는 직전 마이크로 배치와 동일한 작업을 하고, 새로운 개수를 상태 정보의 기존  개수에 더해서 상태 정보를 업데이트한다.

여기서 더 나아가 장애 발생 시 메모리의 정보를 잃을 위험이 있기 때문에 각 배치에 변화된 변경 내용과 오프셋 범위로 체크포인트 로그를 남긴다. 

상태 정보 유지 + 체크포인트 로그를 통해 장애 발생 시에 해당 배치 이전의 동일한 상태 정보로 동일한 입력 데이터를 재처리하며 이는 정확한 일회 실행을 보장하기 위한 핵심적인 요소가 된다.

### 상태 정보 유지 연산의 종류들

스트리밍 상태 정보의 본질은 과거 데이터의 요약 정보를 유지하는 것이며, 때론 새로운 정보를 위해 과거 데이터는 정리할 필요도 있다. 

그래서 두 가지 타입의 상태 정보 유지 연산을 구분할 수 있다.

- 관리형 상태 정보 유지 연산
어떤 연산이 만료 인지 아닌지를 판단해 자동적으로 오래된 상태 정보를 감지하고 정리
    - 스트리밍 집계 연산
    - 스트림-스트림 조인
    - ㅁ
- 비관리형 상태 정보 유지 연산
직접 자신만의 상태 정리 로직을 정의 할 수 있게 해 준다.
아래의 연산들은 원하는대로 임의의 상태 정보 유지 연산을 정의하도록 도와준다.
    - MapGroupsWithState
    - FlatMapGroupsWithState

# 상태 정보 유지 스트리밍 집계

정형화 스트리밍은 대부분의 데이터 프레임 집계 연산을 누적시키면서 실행한다.

다양한 타입의 스트리밍 집계에 대한 배경과 연산, 스트리밍에서 지원하지 않는 집계 연산 형태들도 간략히 살펴본다.

## 시간과 연관 없는 집계

- 전체 집계
스트림으로 들어오는 모든 데이터를 통틀어 집계
스트리밍 데이터 프레임은 집계 결과를 지속적으로 업데이트 하기 때문에 DataFrame.count() 같은 직접적인 연산을 사용할 수 없어 groupBy() 같은 함수들을 사용해야한 다.
    
    ```python
    # In Python
    runningCount = sensorReadings.groupBy().count()
    ```
    
- 그룹화 집계
데이터 스트림에서 나타나는 각 그룹이나 키별로 집계한다.
각 센서별로 실행 중인 평균 값을 아래와 같이 계산 할 수 있다.
    
    ```python
    # In Python
    baselineValues = sensorReadings.groupBy("sensorId").mean("value")
    ```
    

합계나 평균 외에도 스트리밍 데이터 프레임에서는 아래와 같은 집계 연산들을 지원한다.

- 모든 내장 집계 함수들
sum(), mean(), stddev(), countDistinct(), collect_set(), approx_count_dis tinct() 등..
자세한 내용은 API문서 참고
- 함께 계산된 다중 집계연산
아래와 같이 동시에 계산 될 수 있도록 적용 가능
    
    ```python
    # In Python
    from pyspark.sql.functions import * 
    multipleAggs = (sensorReadings
              .groupBy("sensorId")
              .agg(count("*"), mean("value").alias("baselineValue"),
                collect_set("errorCode").alias("allErrorCodes")))
    ```
    
- 사용자 정의 집계 함수
모든 사용자 정의 집계 함수를 사용할 수 있다. 타입 지정/비지정은 가이드를 참고한다.

스트리밍 집계 연산 실행과 관련되어 알아봤으며 이제 시간에 의존하지 않는 집계 연산에 대해 알아볼 것이다.

## 이벤트 타임 윈도우에 의한 집계

시간 범위에 따라 구분된 데이터에 대한 집계가 필요할 수 있다. 

전송 딜레이 때문에 이벤트 타임(읽기가 일어났을 때를 표시하는 레코드 타임스탬프)으로 계산해야하며 5분인 걸로 가정해보자.

```python
# In Python
from pyspark.sql.functions import * 
(sensorReadings
      .groupBy("sensorId", window("eventTime", "5 minute"))
      .count())
```

window()함수가 동적으로 그룹화해 계산하는 칼럼같은 개념이며 5분 간격으로 표현할 수 있게 해준다.

순서는 다음과 같다.

1. eventTime값을 사용하여 5분 단위로 읽어들인 값을 계산한다.
2. 센서ID를 기준으로 그룹화 한다.
3. 그룹의 합계를 갱신한다.

각각의 이벤트 타임을 기반으로 몇몇 센서의 입력값이 어떻게 5분 간격으로 연속해서 들어와 각 그룹에 배치되는지 보여준다.

두개의 시간선은 언제 처리되는지와 이벤트 데이터의 시간을 보여준다

- 그림에서 보면 이벤트 타임 12:07인 이벤트가 실제로는 12:11 이벤트 이후에 도착하고 처리되었다.
- 사실 시간 간격의 정의에 따라 각 이벤트는 여러 그룹에 할당될 수도 있다. 예를 들어 만약 10분
단위의 시간 간격을 5분씩 움직인다고 하면 다음과 같이 할 수 있다.

### 늦게 도착하는 데이터를 위한 워터마크 다루기

- **워터마크** watermark
처리된 데이터에서 쿼리에 의해 검색된 최대 이벤트 시간보다 뒤처지는 이벤트 시간의 동적인 임계값
- **워터마크 지연** watermark delay (뒤처지는 간격)
늦게 도착하는 데이터를 엔진이 얼마나 오래 기다릴 수 있는지를 정의
- 지정 그룹에 대해 데이터가 더 이상 들어오지 않는 시점을 알게 된다면 엔진은 자동적으로 해당 그룹에 대한 처리를 종료하고, 상태 정보에서 삭제해버릴 수 있다. 
⇒ 이런 방식으로 엔진이 쿼리의 결과를 계산하고 유지해야하는 상태 정보의 양을 줄일 수 있다.
- 예를 들어 센서 데이터가 10분 이상 지연되지 않는다는 것을 알고 있다고 하면, 워터마크를 다음과 같이 세팅할 수 있다.

- 워터마크가 제공해주는 **“보장”**의 정확한 의미
    - 10분짜리 워터마크: 엔진이 입력 데이터에 나타난 가장 최근 이벤트 시간과 비교해,
    10분 이내로 지연된 데이터는 절대 버리지 않는다는 것을 보장 → 오직 한 방향으로만 적용
    - 10분 이상 지연된 데이터는 반드시 삭제되는 것은 아님
    - 10분 이상 지연된 입력 레코드가 집계될지 아닐지는 레코드가 언제 도착하는지에 대한
    정확한 타이밍과 마이크로 배치 처리가 언제 시작되는지에 따름

### 지원되는 출력 모드들

- 시간과 상관없는 스트리밍 집계와는 달리, 타임 윈도우가 포함되는 집계는 3가지 출력 모드 가능
    - 해당 모드별로 상태 정보 초기화와 관련한 묵시적인 사항 존재

### 갱신 모드

- 모든 마이크로 배치가 집계 갱신된 부분의 열만 출력 → 모든 타입의 집계에 사용 가능
- 특히 타임 윈도우 집계에서 워터마킹은 상태 정보가 정기적으로 초기화되도록 해줌
    - 스트리밍 집계 쿼리 실행에 가장 유용
    - But, Parquet/ORC 파일 기반 포맷 같은 추가 전용 스트리밍 싱크에 집계를 출력하는 용도로는 사용 불가 → Delta Lake를 쓰면 가능함

### 전체 모드

- 모든 마이크로 배치는 모든 갱신된 집계를 변화가 있는지 없는지, 얼마나 오래되었는지에 상관 없이 출력함
- 모든 타입 집계에 사용가능하지만, 타임 윈도우 집계에 사용하는 것은 워터마크가 지정되어 있더라도 상태 정보가 초기화되지 않음
- 모든 집계를 출력하는 것은 모든 과거 상태 정보를 필요로 하므로, 워터마크가 지정되어 있더라도 집계 데이터가 계속 보존되어야 함
    - 상태 정보 크기와 메모리 사용량이 무한하게 증가할 수 있다는 여지 → 타임 윈도우에서 조심

### 추가 모드

- **워터마크를 쓰는 이벤트 타임 윈도우 집계에 대해서만** 사용 가능
    - 이전 출력 결과 변경을 허용하지 않음
    - 워터마크가 없는 집계라면, 나중에 어떤 데이터가 와서 갱신될지 알 수 없으므로 추가 모드에서 출력될 수 없음
    - 오직 워터마크가 활성화된 이벤트 타임 윈도우 집계에서만 어느 시점에 더이상 쿼리 집계가 갱신되지 않을지 알 수 있음
- 파일 같은 추가 전용 스트리밍 싱크에 집계 내용을 쓸 수 있으나, 워터마크 시간만큼 출력도 늦음
    - 쿼리는 이어지는 워터마크가 집계가 아직 완료되지 않은 해당 키의 시간 간격을 지나기까지 기다릴 필요가 있기 때문

데이터 스트림을 정적 데이터세트와 조인하는 경우

- 예시: 노출되는 광고 정보 데이터세트 Impression(정적)과 광고를 사용자들이 클릭했을 때 발생하는 이벤트 정보 데이터세트 Click(스트림)이 있다고 가정
- 두 종류의 데이터 프레임(정적인 데이터와 스트리밍 데이터)으로 데이터 표현

- 두 개의 정적 데이터 프레임을 조인할 때와 동일한 코드
- 실행 시 클릭에 대한 모든 마이크로배치는 정적 테이블과 내부 조인되어 서로 맞는 이벤트의 스트림을 출력
- 그 외에 가능한 조인 형태
    - **왼쪽이 스트리밍** 데이터 프레임일 때의 **좌측 외부 조인**

- **오른쪽이 스트리밍** 데이터 프레임일 때의 **우측 외부 조인**
- 그 외의 외부 조인은 점진적으로 실행하기 어려워서 지원되지 않음

스트림-정적 조인에서 알아두어야 할 사항

- 스트림-정적 조인은 **무상태 연산**이므로 **워터마킹이 필요하지 않음**
- **정적 데이터 프레임**은 스트리밍 데이터의 마이크로 배치마다 조인되면서 **반복적으로 읽힘**
    - 따라서 처리 속도를 올리고 싶으면 **캐시**를 해야 함
- 정적 데이터 프레임이 정의된 데이터 소스에서의 데이터가 변경된 경우, **변경사항**이 스트리밍 쿼리에서 보일지는 **데이터 소스에서 지정된 정책**에 달려 있음
    - 정적 데이터 프레임이 파일에 정의되어 있다면 파일에 대한 변경(예: 추가)은 스트리밍 쿼리가 재시작되기 전까지 반영되지 않을 것

### 스트림-스트림 조인

두 개의 데이터 스트림 사이에서 지속적으로 조인을 수행하는 것

- 문제점: 어느 시점에서든 한쪽 데이터세트의 상태가 불완전 → 입력값 사이에서 매칭되는 것을 찾는 것이 더 어려움
    - 두 스트림에서 매칭되는 이벤트들이 어떤 순서로 올지, 얼마나 지연될지도 알 수 없음
- 정형화 스트리밍은 양쪽 스트리밍 상태로부터 **입력 데이터를 버퍼링**해 지연을 처리하고, 지속적으로 **새로운 데이터가 도착할 때마다 매칭되는지 체크**

선택적 워터마킹을 사용한 내부 조인

- impressions 데이터 프레임을 스트리밍 데이터로 재정의했다고 가정

```python
# 스트리밍 데이터 프레임 [adId: String, impressionTime: Timestamp, ...]
impressionsStatic = spark.readStream. ...

# 스트리밍 데이터 프레임 [adId: String, clickTime: Timestamp, ...]
clickStream = spark.readStream. ...
```

- 엔진은 모든 클릭과 임프레션 정보를 상태 정보에 버퍼링
 → 버퍼링된 임프레션과 매칭되는 클릭 정보를 받거나 그 반대의 경우에 매칭되는 임프레션과 클릭 정보 생성

스트림-스트림 조인에서 **유지되는 상태 정보를 제한**하기 위해서는 사용 패턴에 대해 다음 정보들을 알아두어야 함

- 각각 데이터 소스에서 **두 이벤트가 생성되는 시간 차이**가 최대 얼마나 되는가?
- 데이터 소스에서 처리 엔진까지 하나의 이벤트는 **최대 얼마나 지연**될 수 있는가?

- 지연한 및 이벤트 타임 제한들 표현 가능(데이터 프레임 연산 안에서 워터마크/ 시간 범위 조건 등으로) ⇒ 상태 정보 깔끔하게 유지하기 위한 절차
1. 양쪽 입력 엔진에 얼마나 지연된 입력을 허용할 수 있도록 워터마크 지연 정의
2. 두 입력 간에 이벤트 타임 제한 정의 (엔진 한쪽 입력의 오래된 레코드가 다른 쪽 입력에 언제 없어질지 알게함)
a. 시간 범위에 대한 조인 조건 
⇒ (”leftTime BETWEEN rightTime AND rightTime + INTERVAL 1 HOUR”)
b. 이벤트 타임 윈도우로 조인
⇒ (”leftTimeWindow = rightTimeWindow”)

- 내부 조인에 대한 키포인트

1) 내부 조인에서 워터마킹, 이벤트 타임 제한 지정은 선택 사항이다
(잠재적으로 연결되지 않은 상태 정보 존재에 대한 위험 감수하더라도 제한을 선택하지 않을 수 있다)

2) 엔진이 2시간 이하로 지연되는 데이터는 잃어버리지 않는다

### 워터마킹을 이용한 외부 조인

- 앞서 살펴본 내부 조인은 클릭 정보가 존재하지 않는 광고에 대해서는 전혀 알 수 없다
- 추후 분석을 위한 용도로 클릭 정보가 있든 없든 모든 광고 Impression 정보에 대해 받아 보기를 원할 수도 있음 ⇒ **스트림ㅡ스트림 외부 조인** 으로 해결
- 외부 조건 타입으로 지정
    
    #파이썬 예제
    
    # 시간 범위 조건으로 left Outer 조인
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8699e8d4-b7db-4f18-aeb2-beb9dd155804/Untitled.png)
    
    # 스칼라 예제
    
    # 시간 범위 조건으로 left Outer 조인
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/816266e0-ddcc-442a-9134-67a70b48364e/Untitled.png)
    
    # 유일한 변경 : 외부 조인 타입 지정
    
    ⇒ 모든 Impression에 대해 클리 정보가 있든 없든 결과를 출력
    

- 외부 조인 알아둘 사항

1) 외부 조인에서는 워터마크 지연이나 이벤트 타임 제한이 선택이 아닌 필수이다
: NULL을 결과로 출력하기 위해서 엔진이 어느 시점부터 이벤트가 매치되지 않을지 알아야 하기 떄문

2) 외부 조인의 NULL 결과 출력을 위해서 엔진은 매치되는 정보가 확실히 없다는 것을 보장하기 위해 대기할 필요가 있다

: 이 지연은 버퍼링 시간이며, 각 이벤트마다 엔진에 의해 계산된다

## 임의의 상태 정보 유지 연산

많은 사용 패턴들 ⇒ SQL 연산자/함수들보다 복잡한 논리 흐름을 필요함

Ex) 실시간 사용자 행동 추적하여 상태 추적하고 싶다고 가정

1) 임의의 자료구조로 상태 정보에 사용자 행동 기록 추적

2) 지속적으로 사용자의 행동에 기반한 자료 구조에 복잡한 로직을 적용해야 함

: mapGroupWithState(), flatMapGroupWithState() 위의 복잡한 분석 사례를 커버하기 위해 설계 됨

스트리밍 쿼리가 시작되면 각 마이크로 배치에서 스파크는 arbitraryStateUpdateFunction()를 데이터의 각 개별 키마다 호출

### 파라미터

- key : K

상태 정보화 입력 데이터에 정의되는 공통 키의 데이터 타입

스파크는 데이터의 각 개별 키마다 이 함수를 호출

- newDataForKey : Iterator[V]

V는 입력 데이터 세트의 데이터 타입.

스파크가 키에 대해서 이 함수를 호출할 때, 이 파라미터는 키와 관련된 모든 입력 데이터를 갖게 됨.

이터레이터에서 가져오는 입력 데이터 객체의 순서는 정해져 있지 않음

- previousStateForKey : GroupState[S]

S는 개발자가 관리하게 될 임의의 상태 정보의 데이터 타입.

GroupState[S]는 상태 정보값들에 접근하고, 관리할 수 있는 함수들을 제공하는 타입을 갖는 포장 객체.

스파크가 키에 대해 함수를 호출할때 이 객체는 이전 스파크가 호출했던 상태값 집합을 제공

- U

함수의 출력값의 데이터 타입

### 프로그래밍 관점에서 다음 단계에 따라 상태 갱신 함수를 정의

1. 데이터 타입 정의

  K,V,S,U에 대한 정확한 타입 정의

- 입력데이터(V) = case class UserAction(userId: String, action: String)
- 키(K) = String *userId
- 상태(S) = case class UserState(userId:String, active =Boolean)
- 결과(U) = UserStatus, 최신 사용자 상태 정보를 담는 클래스

2. 함수 정의

- 새로운 함수가 사용자 행동과 동시에 호출될 때 대응해야 할 두 가지의 주요 상황
- 키에 해당하는 이전 상태 정보가 존재하는지 아닌지
- 어느 쪽인지에 따라 적절하게 새로운 행동에 대한 정보와 함께 현재 상태를 갱신해주거나, 아니면 사용자 상태 정보를 초기화
- 명시적으로 상태를 새로운 집계와 함께 갱신해주고, 최종적으로는 갱신된 userId-userStatus 쌍을 되돌려줌

### 비활성화 그룹을 관리하기 위한 타임아웃 사용

- 활성 사용자 세션 추적 예제에서는 사용자가 수가 증가할수록 관리해야 할 상태 정보의 키 숫자가 늘어날 뿐만 아니라 상태 정보 유지를 위한 메모리 사용량도 늘어날 수 있다.
- 실제로 사용자들은 언제나 활성(active) 상태로 남아 있지 않기 때문에 그 정보를 계속 유지하는 것은 도움이 되지 않을 수 있다.
- 따라서 비활성화 사용자들에 대한 정보를 명시적으로 제거하는 것이 필요할 수 있지만, 사용자가 어떤 상태에서 비활성화 인지는 알 수 없다.
- 시간 기반 비활성화를 인코딩하기 위해 `mapGroupsWithState()` 는 다음처럼 정의되는 타음아웃 개념을 지원한다.
    1. 매번 함수는 하나의 키에 대해 호출되며 타임아웃은 하나의 키에 대해 시간 기반이나 임계값의 타임스탬프를 기반으로 설정할 수 있다.
    2. 만약 키에 대한 데이터가 전혀 들어오지 않고 해당 타임아웃 조건을 만족한다면 키는 ‘타임 아웃됨’으로 표시된다. 
        1. 다음 마이크로 배치는 이 키에 대한 데이터가 전혀 없더라도 이 타임아웃 키에 대해 함수를 호출할 수 있다. 
- 타임 아웃에는 처리 시간과 이벤트 타임에 따른 두 가지 종류가 있다.

### 처리 시간 타임아웃

- 처리 시간 타임아웃은 스트리밍 쿼리가 도는 머신에서의 시스템 시간 기준으로 한다.
    - 한 키에 대해 마지막으로 데이터를 시스템 시간 T에 받았으며 현재 시각이 `(T + <타임아웃 시간>)` 보다 크다면 새로운 빈 데이터 이터레이터와 함께 함수가 호출될 것이다.

**이벤트 타임 타임아웃**

이벤트 타임아웃은 데이터에 있는 이벤트 타임과 이벤트 타임에 저장된 워터마크 이용.

만약 한 키에 특정 타임아웃 타임스탬프T가 설정되어 있고, 그 키에 대한 데이터가 함수가 호출된 이후 새로운 데이터가 없는 채로 워터마크가 T를 초과 한다면, 키는 타임아웃 처리

**워터마크** 

데이터가 처리되는 동안 가장 최신 이벤트 타임 이후의 지연된 시간을 의미하는 동적인 임계값.

타임아웃 시점은 현재 워터마크보다는 큰 값으로 지정되어야 함. 

**flatMapGroupsWithState()를 사용한 일반화**

매번 mapGroupWithState()가 호출될 때 오직 하나의 레코드를 되돌려주어야 함.

mapGroupWithState()로는 불투명한 상태 정보 업데이트 함수에 대한 정보 부재로 엔진은 생성된 레코드들이 갱신된 키/값 데이터 쌍이라고 가정. 그에따라 다운스트림 연산에 대해 판단하고 어떤 것을 허용할지, 하지 않을지 결정.

→flatMapGroupsWithStates는 조금 더 문법이 복잡하지만 위의 제한들을 극복함. mapGroupsWithstate와 두가지 차이점이 있다.

1. 리턴타입이 단일 객체가 아니라 이터레이터. 함수가 레코드 개수와 관계없이 리턴할 수 있게 하며, 심지어 아무것도 리턴하지 않는 것도 가능. 
2. 연산자 출력 모드라고 불리는 또 다른 파라미터를 받아들이는데, 이는 출력 레코드가 추가될 수 있는 새로운 레코드인지 갱신된 키/값 레코드 인지를 정의한다.

### 성능튜닝

정형화 스트리밍은 Spark SQL 엔진을 사용하므로 5,7장에서와 동일한 파라미터들로 튜닝할 수 있다. 그러나 “배치 잡”과 달리 적은 데이터양을 처리하는 “마이크로 배치 잡”이므로 몇 가지 고민해봐야할 포인트들이 있다.

1. **클러스터 자원 배치**
    1. 24/7로 클러스터들이 돌아가기 때문에 적절한 자원배치가 중요함. 자원이 부족하게 배치되면 쿼리지연이 일어날 수 있고, 과하게 배치하면 쓸모없는 비용이 발생한다. 
    2. 스트리밍 쿼리의 특성에 맞는 자원배치해야한다. **무상태 쿼리(이전 쿼리 결과에 의존하지 않는 쿼리)에는 더 많은 코어**가 필요하고, **상태 정보 유지 쿼리(이전 쿼리 결과에 의존하는 쿼리)는 메모리를 더 필요**로 한다.
    
    ** 24/7 로 클러스터가 돌아간다는 의미는 하루 24시간, 일주일 7일 즉, 연중무휴로 운영된다는 의미
    
    - 스트리밍 쿼리에서 자원 배치하는 순서
        1. **쿼리의 유형을 식별**합니다. 쿼리가 무상태인지 상태 정보 유지인지 확인합니다.
        2. **쿼리의 특성을 식별**합니다. 쿼리에 어떤 데이터가 필요하고 얼마나 많은 데이터가 필요합니까? 쿼리는 얼마나 자주 실행됩니까?
        3. 쿼리에 **필요한 리소스를 결정**합니다. 쿼리에 얼마나 많은 코어와 메모리가 필요합니까?
        4. **리소스를 쿼리에 할당**합니다. 스파크 클러스터에 리소스를 할당합니다.
        5. 쿼리를 실행하고 **성능을 모니터링**합니다. 쿼리의 성능을 모니터링하고 필요에 따라 리소스를 조정합니다.
2. **셔플을 위한 파티션 숫자**
    1. 정형화 스트리밍 쿼리에선 대개 배치 쿼리보다는 셔플 파티션 갯수를 적게한다. 그러나 너무 잘개 쪼개면 오히려 오버헤드를 일으키고 처리량을 감소 시킬 수 있다.  (상태 정보 유지 연산에 의한 셔플은 체크포인팅때문에 더 큰 오버헤드 발생)
    2. 만약 **상태 정보 유지 연산 + second/minute 단위 트리거 간격의 스트리밍 쿼리**라면 셔플 파티션 숫자를 **기본(200) 대비 2-3배로 수정**하는 걸 권한다.
3. **안정성을 위한 소스의 처리량 제한**
    1. 자원 할당과 설정이 쿼리에 최적화된 후라도, 갑작스레 데이터 처리량 증가로 클러스터가 불안정해질 수 있다. 이때, 소스 전송률 제한으로 불안정성에 대한 방어가 가능하다. (자원 과다 할당도 하나의 방법이지만 비용이 많이 듦)
    2. 다만 너무 제한을 낮게 설정하면 쿼리가 할당 자원을 제대로 쓰지 못할 수 있고, 입력 비율이 계속해서 올라갈 경우 효과적으로 처리하지 못할 수 있다. 소스의 안정성이 유지되더라도, 처리되지 않은 데이터는 소스에서 무한하게 증가하여 응답속도를 느리게할 수 있다. 그러므로, **소스 전송률 제한이 소스에서 생성되는 데이터 속도를 초과하지 않도록 설정해야함. (데이터 생성 속도 제한 or 더 작은 chunk로 분할)**
4. **동일 스파크 애플리케이션에서 다중 스트리밍 쿼리 실행**
    1. 동일한 SparkContext , SparkSession 내에서 여러개의 스트리밍 쿼리를 실행하는 것은 자원을 세분화하여 공유할 수 있다. 다만 아래 2가지를 참고해야한다.
        1. 각 쿼리를 지속적으로 실행하는 것은 스파크 드라이버 자원(JVM 돌아가는 곳)을 사용하게 되는데, 이는 동시 실행 쿼리수를 제한한다. 만약 초과한다면 **테스크 스케줄링이 병목(executor  활용 x)되거나 메모리 제한을 넘게될 수** 있다.
        2. 각 쿼리 별로 별도의 스케줄러 풀에서 돌도록 설정하여 동일한 컨텍스트 내에서 쿼리 사이에 안정적 자원 할당이 이뤄지도록 한다. 
            - SparkContext의 스레드 로컬 속성인 spark.scheduler.pool에 서로 다른 문자열 값으로 각 스트림마다 지정해준다.
