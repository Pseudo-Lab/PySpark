{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#스트리밍 쿼리를 정의하는데 필요한 다섯 단계\n",
    "\n",
    "#1.입력소스지정\n",
    "spark = SparkSession…\n",
    "\n",
    "lines = (spark.readStream.format(“socket”)\n",
    "\n",
    ".option(“host”,”localhost”)\n",
    "\n",
    ".option(“port”,9999)\n",
    "\n",
    ".load())\n",
    "\n",
    "\n",
    "#2. 데이터변형\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "words = lines.select(split(col(“value”), “\\\\s”).alias(“word”))\n",
    "\n",
    "counts = words.groupBy(“word”).count()\n",
    "\n",
    "\n",
    "#3. 출력 싱크와 모드 결정\n",
    "writer = counts.writeStream.format(\"console\").outputMode(\"complete\")\n",
    "\n",
    "\n",
    "#4. 처리 세부사항 결정\n",
    "checkpointDir = '/path'\n",
    "writer2 = (writer\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\", checkpointDir))\n",
    "\n",
    "#5. 쿼리시작\n",
    "streamingQuery = writer2.start()\n",
    "\n",
    "##종합예제\n",
    "##소켓에서 텍스트데이터 스트림을 읽어들이고, 단어 갯수를 세어서 콘솔에 출력하는 코드\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# 1단계: 입력 소스 지정\n",
    "spark = SparkSession...\n",
    "lines = (spark\n",
    "         .readStream.format(\"socket\")\n",
    "         .option(\"host\", \"localhost\")\n",
    "         .option(\"port\", 9999)\n",
    "         .load())\n",
    "\n",
    "# 2단계: 데이터 변형\n",
    "words = lines.select(split(col(\"value\"), \"\\\\s\").alias(\"word\"))\n",
    "counts = words.groupBy(\"word\").count()\n",
    "checkpointDir = \"...\"\n",
    "\n",
    "# 3단계: 출력 싱크와 모드 결정 (전체 모드)\n",
    "# 4단계: 처리 세부사항 지정\n",
    "streamingQuery = (counts\n",
    "                  .writeStream\n",
    "                  .format(\"console\")\n",
    "                  .outputMode(\"complete\")\n",
    "                  .trigger(processingTime=\"1 second\")\n",
    "                  .option(\"checkpointLocation\", checkpointDir)\n",
    "                  .start())\n",
    "\n",
    "# 5단계: 쿼리 시작\n",
    "streamingQuery.awaitTermination()\n",
    "\n",
    "\n",
    "\n",
    "#쿼리 재시작 사이 쿼리를 수정하는 법\n",
    "##ex) 입력중 잘못된 바이트 배열은 무시하도록 추가.\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "### 잘못된 바이트 배열을 무시하기 위한 udf 함수\n",
    "def isCorruptedUdf(value):\n",
    "    return False\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"CountWord\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.udf.register(\"isCorruptedUdf\", isCorruptedUdf, BooleanType())\n",
    "\n",
    "lines = (spark\n",
    "         .readStream.format(\"socket\")\n",
    "         .option(\"host\", \"localhost\")\n",
    "         .option(\"port\", 9999)\n",
    "         .load())\n",
    "\n",
    "# add transformation\n",
    "filteredLines = lines.filter(\"isCorruptedUdf(value) = False\")\n",
    "\n",
    "\n",
    "# words = lines.select(split(col(\"value\"), \"\\\\s\").alias(\"word\"))\n",
    "words = filteredLines.select(split(col(\"value\"), \"\\\\s\").alias(\"word\"))\n",
    "counts = words.groupBy(\"word\").count()\n",
    "checkpointDir = \"...\"\n",
    "\n",
    "\n",
    "streamingQuery = (counts\n",
    "                  .writeStream\n",
    "                  .format(\"console\")\n",
    "                  .outputMode(\"complete\")\n",
    "                  .trigger(processingTime=\"1 second\")\n",
    "                  .option(\"checkpointLocation\", checkpointDir)\n",
    "                  .start())\n",
    "\n",
    "streamingQuery.awaitTermination()\n",
    "\n",
    "\n",
    "#소스와 싱크 옵션\n",
    "#ex) 매 트리거마다 100줄씩 출력하는 옵션을 추가하려면\n",
    "\n",
    "writeStream.format(\"console\").option(\"numRows\", \"100\"),,,\n",
    "\n",
    "\n",
    "#파일 기반의 정형화 스트리밍 처리 방법\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "inputDirectoryOfJsonFiles = ...\n",
    "fileSchema = (StructType()\n",
    " .add(StructField(\"key\", IntegerType()))\n",
    " .add(StructField(\"value\", IntegerType())))\n",
    "inputDF = (spark\n",
    " .readStream\n",
    " .format(\"json\")\n",
    " .schema(fileSchema)\n",
    " .load(inputDirectoryOfJsonFiles))\n",
    "\n",
    "\n",
    "#파일에 쓰기 예제\n",
    "outputDir = ...\n",
    "checkpointDir = ...\n",
    "resultDF = ...\n",
    "streamingQuery = (resultDF.writeStream\n",
    " .format(\"parquet\")\n",
    " .option(\"path\", outputDir) #\"path\" 옵션 대신 직접적으로 start(outputDir)에 지정할 수도 있다.\n",
    " .option(\"checkpointLocation\", checkpointDir)\n",
    " .start())\n",
    "\n",
    "\n",
    "#카프카에서 읽기\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                .config(\"spark.jars\", \"./jar/mysql-connector-j-8.0.32.jar\") \\\n",
    "                .master(\"local\") \\\n",
    "                .appName(\"mysql\") \\\n",
    "                .getOrCreate()\n",
    "\n",
    "inputDF = (spark\n",
    "           .readStream\n",
    "           .format(\"kafka\")\n",
    "           .option(\"kafka.bootstrap.servers\", \"host1:port1\")\n",
    "           .option(\"subscribe\", \"events\")\n",
    "           .load())\n",
    "\n",
    "#카프카에서 쓰기\n",
    "counts = ...\n",
    "streamingQuery = (counts.\n",
    "                  selectExpr(\"cast(word as string) as key\",\n",
    "                             \"cast(count as string) as value\")\n",
    "                  .writeStream\n",
    "                  .format(\"kafka\")\n",
    "                  .option(\"kafka.bootstrap.servers\", \"host1:port1\")\n",
    "                  .option(\"topic\", \"wordCounts\")\n",
    "                  .outputMode(\"update\")\n",
    "                  .option(\"checkpointLocation\", checkpointDir)\n",
    "                  .start())\n",
    "\n",
    "#임의의 저장 시스템에 쓰기\n",
    "hostAddr = \"<ip address>\"\n",
    "keyspaceName = \"<keyspace>\"\n",
    "tableName = \"<tableName>\"\n",
    "\n",
    "spark.conf.set(\"spark.cassandra.connection.host\", hostAddr)\n",
    "\n",
    "def writeCountsToCassandra(updatedCountsDF, batchId):\n",
    "    (updatedCountsDF\n",
    "     .write\n",
    "     .format(\"org.apache.spark.sql.cassandra\")\n",
    "     .mode(\"append\")\n",
    "     .options(table=tableName, keyspace=keyspaceName)\n",
    "     .save())\n",
    "\n",
    "streamingQuery = (counts\n",
    "                  .writeStream\n",
    "                  .foreachBatch(writeCountsToCassandra)\n",
    "                  .outputMode(\"update\")\n",
    "                  .option(\"checkpointLocation\", checkpointDir)\n",
    "                  .start())\n",
    "\n",
    "##foreach사용\n",
    "## 1. 함수사용\n",
    "def process_row(row):\n",
    "\t# 이제 저장장치에 쓴다.\n",
    "\tprint(\"Processing row:\", row)\n",
    "\n",
    "query = streamingDF.writeStream.foreach(process_row).start()\n",
    "\n",
    "\n",
    "## 2. ForeachWriter 클래스 사용\n",
    "class ForeachWriter:\n",
    "\tdef open(self, partitionId, epochId):\n",
    "\t# 데이터 저장소에 대한 접속을 열어놓는다.\n",
    "\t# 쓰기가 계속되어야 하면 true를 리턴한다.\n",
    "\t# 파이썬에서는 이 함수는 선택 사항이다.\n",
    "\t# 지정되어 있지 않다면 자동적으로 쓰기는 계속될 것이다.\n",
    "\treturn true\n",
    "\n",
    "def process(self,row):\n",
    "\t#열린 접속을 사용해서 저장소에 문자열을 쓴다.\n",
    "\t# 이 함수는 필수\n",
    "\tprint(\"Processing row:\", row)\n",
    "\n",
    "def close(self, error):\n",
    "\t# 접속을 닫는다. 이 함수는 선택\n",
    "\tpass\n",
    "\n",
    "resultDF.writeStream.foreach(ForeachWriter()).start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
