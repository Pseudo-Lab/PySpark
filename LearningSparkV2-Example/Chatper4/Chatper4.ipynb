{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Session 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession\n",
    "      .builder\n",
    "      .appName(\"Chatper4\")\n",
    "      .getOrCreate())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 쿼리 예제 88p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로\n",
    "csv_file = \"PySpark/data/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "\n",
    "# 읽고 임시뷰를 생성\n",
    "# 스키마 추론(더 큰 파일일 경우에는 스키마를 지정해주자)\n",
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .load(csv_file))\n",
    "\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 90p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT distance, origin, destination\n",
    "    FROM us_delay_flights_tbl WHERE distance > 1000\n",
    "    ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "    FROM us_delay_flights_tbl\n",
    "    WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "    ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 91p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "              CASE\n",
    "                  WHEN delay > 360 THEN 'Very Long Delays'\n",
    "                  WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "                  WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "                  WHEN delay > 0 and delay < 60  THEN  'Tolerable Delays'\n",
    "                  WHEN delay = 0 THEN 'No Delays'\n",
    "                  ELSE 'Early'\n",
    "               END AS Flight_Delays\n",
    "               FROM us_delay_flights_tbl\n",
    "               ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 92p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "from pyspark.sql.functions import col, desc \n",
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    "      .where(col(\"distance\") > 1000)\n",
    "      .orderBy(desc(\"distance\"))).show(10)\n",
    "# Or\n",
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    "    .where(\"distance > 1000\")\n",
    "    .orderBy(\"distance\", ascending=False).show(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 93p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// In Scala/Python\n",
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 94p 기존 코드 오류로 인한 아래 코드로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # // In Scala/Python 기존 코드\n",
    "# spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오류로 인한 수정한 코드\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Chatper4\").config(\"spark.sql.legacy.createHiveTableByDefault\", \"false\").getOrCreate()\n",
    "\n",
    "#// In Scala/Python\n",
    "spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT,distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "# Path to our US flight delays CSV file\n",
    "csv_file = \"PySpark/data/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\" # Schema as defined in the preceding example\n",
    "schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\" \n",
    "flights_df = spark.read.csv(csv_file, schema=schema) \n",
    "\n",
    "# 위에서 만든 테이블 삭제\n",
    "spark.sql(\"DROP TABLE IF EXISTS managed_us_delay_flights_tbl\")\n",
    "flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 만든 테이블 삭제\n",
    "spark.sql(\"DROP TABLE IF EXISTS us_delay_flights_tbl\")\n",
    "\n",
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT,distance INT, origin STRING, destination STRING)\n",
    "    USING csv OPTIONS (\n",
    "    PATH \"PySpark/data/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\")\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 덮어쓰기\n",
    "spark.conf.set(\"spark.sql.legacy.allowNonEmptyLocationInCTAS\", \"true\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS us_delay_flights_tbl\")\n",
    "\n",
    "(flights_df\n",
    "      .write\n",
    "      .option(\"path\", \"/tmp/data/us_flights_delay\")\n",
    "      .saveAsTable(\"us_delay_flights_tbl\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 95p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- In SQL\n",
    "spark.sql(\"\"\"CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE origin = 'SFO'\"\"\")\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE origin = 'JFK'\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 96p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
    "# Create a 전역 임시 and 임시 뷰\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- In SQL\n",
    "spark.sql(\"\"\"SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view\"\"\")\n",
    "spark.sql(\"\"\"SELECT * FROM us_origin_airport_JFK_tmp_view\"\"\")\n",
    "# // In Scala/Python\n",
    "spark.read.table(\"us_origin_airport_JFK_tmp_view\")\n",
    "# // Or\n",
    "spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -- In SQL\n",
    "# DROP VIEW IF EXISTS us_origin_airport_SFO_global_tmp_view; \n",
    "# DROP VIEW IF EXISTS us_origin_airport_JFK_tmp_view\n",
    "\n",
    "# // In Scala/Python\n",
    "spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 97p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  // In Scala/Python\n",
    "spark.catalog.listDatabases()\n",
    "spark.catalog.listTables()\n",
    "spark.catalog.listColumns(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -- In SQL\n",
    "# # 테이블에 캐싱하기\n",
    "# CACHE [LAZY] TABLE <table-name> \n",
    "# UNCACHE TABLE <table-name>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 98p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_flights_df = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\n",
    "us_flights_df2 = spark.table(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 102p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "file = \"\"\"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\"\"\"\n",
    "df = spark.read.format(\"parquet\").load(file)\n",
    "df.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 103p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- In SQL\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl \n",
    "    USING parquet \n",
    "    OPTIONS (\n",
    "        path \"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\" )\n",
    "\"\"\")\n",
    "\n",
    "# In Python\n",
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)\n",
    "\n",
    "# # DataFrame으로 테이블을 읽어옵니다.\n",
    "# us_delay_flights_df = spark.read.format(\"parquet\").option(\"path\", \"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\").load()\n",
    "\n",
    "# # 임시 뷰를 생성합니다.\n",
    "# us_delay_flights_df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 104p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "(df.write.format(\"parquet\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .save(\"/tmp/data/parquet/df_parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "(df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"us_delay_flights_tbl\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 105p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "file = \"/Users/hyunjun/vscode/Spark//databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\n",
    "df = spark.read.format(\"json\").load(file)\n",
    "df.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 106p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- In SQL\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl USING json\n",
    "    OPTIONS (\n",
    "    path \"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "#// In Scala/Python\n",
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "(df.write.format(\"json\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .save(\"/tmp/data/json/df_json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 108p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "file = \"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\"\n",
    "schema = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\"\n",
    "df = (spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")# Exit if any errors .option(\"nullValue\", \"\")\n",
    "    .schema(schema)# Replace any null data field with quotes \n",
    "    .option(\"mode\", \"FAILFAST\")\n",
    "    .load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- In SQL\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl USING csv\n",
    "OPTIONS (\n",
    "path \"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\", header \"true\",\n",
    "inferSchema \"true\",\n",
    "mode \"FAILFAST\"\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# // In Scala/Python\n",
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 109p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/data/csv/df_csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 110p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"avro\").load(\"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 111p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- In SQL\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW episode_tbl USING avro\n",
    "OPTIONS (\n",
    "path \"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\"\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "spark.sql(\"SELECT * FROM episode_tbl\").show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 112p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "(df.write\n",
    "  .format(\"avro\")\n",
    "  .mode(\"overwrite\")\n",
    "  .save(\"/tmp/data/avro/df_avro\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 113p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "file = \"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\n",
    "df = spark.read.format(\"orc\").option(\"path\", file).load()\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- In SQL\n",
    "spark.sql(\n",
    "    \"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl \n",
    "    USING orc\n",
    "    OPTIONS (\n",
    "    path \"PySpark/data/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\n",
    "    )\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# // In Scala/Python\n",
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "(df.write.format(\"orc\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .save(\"/tmp/data/orc/flights_orc\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 116p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "from pyspark.ml import image\n",
    "image_dir = \"PySpark/data/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\n",
    "images_df = spark.read.format(\"image\").load(image_dir)\n",
    "images_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df.select(\"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\",\n",
    "      \"label\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "path = \"PySpark/data/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\n",
    "binary_files_df = (spark.read.format(\"binaryFile\")\n",
    "    .option(\"pathGlobFilter\", \"*.jpg\")\n",
    "    .load(path))\n",
    "binary_files_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "binary_files_df = (spark.read.format(\"binaryFile\")\n",
    "    .option(\"pathGlobFilter\", \"*.jpg\")\n",
    "    .option(\"recursiveFileLookup\", \"true\")\n",
    "    .load(path))\n",
    "binary_files_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15829a5d5b0063a6cb8d1011ad4433a337dc97bb73b11e2356c031cceb4e0399"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
